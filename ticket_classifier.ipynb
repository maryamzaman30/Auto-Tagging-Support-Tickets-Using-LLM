{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Tagging Support Tickets Using LLM\n",
    "\n",
    "This notebook demonstrates auto-tagging of support tickets using zero-shot, few-shot and fine-tuned approaches with Hugging Face transformers.\n",
    "\n",
    "We define categories based on the dataset, manually label for evaluation/fine-tuning, compare performances and save a fine-tuned model for app.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  support_tick_id                                support_ticket_text\n",
      "0      ST2023-006  My internet connection has significantly slowe...\n",
      "1      ST2023-007  Urgent help required! My laptop refuses to sta...\n",
      "2      ST2023-008  I've accidentally deleted essential work docum...\n",
      "3      ST2023-009  Despite being in close proximity to my Wi-Fi r...\n",
      "4      ST2023-010  My smartphone battery is draining rapidly, eve...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset/support_ticket_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories based on dataset analysis\n",
    "categories = [\n",
    "    \"Connectivity Issue\",\n",
    "    \"Hardware Malfunction\",\n",
    "    \"Data Recovery\",\n",
    "    \"Battery Issue\",\n",
    "    \"Account Access\",\n",
    "    \"Performance Issue\",\n",
    "    \"Software Issue\"\n",
    "]\n",
    "\n",
    "# Manual labels for evaluation and fine-tuning\n",
    "labels_dict = {\n",
    "    \"ST2023-006\": \"Connectivity Issue\",\n",
    "    \"ST2023-007\": \"Hardware Malfunction\",\n",
    "    \"ST2023-008\": \"Data Recovery\",\n",
    "    \"ST2023-009\": \"Connectivity Issue\",\n",
    "    \"ST2023-010\": \"Battery Issue\",\n",
    "    \"ST2023-011\": \"Account Access\",\n",
    "    \"ST2023-012\": \"Performance Issue\",\n",
    "    \"ST2023-013\": \"Hardware Malfunction\",\n",
    "    \"ST2023-014\": \"Data Recovery\",\n",
    "    \"ST2023-015\": \"Hardware Malfunction\",\n",
    "    \"ST2023-016\": \"Data Recovery\",\n",
    "    \"ST2023-017\": \"Hardware Malfunction\",\n",
    "    \"ST2023-018\": \"Hardware Malfunction\",\n",
    "    \"ST2023-019\": \"Data Recovery\",\n",
    "    \"ST2023-020\": \"Hardware Malfunction\",\n",
    "    \"ST2023-021\": \"Connectivity Issue\",\n",
    "    \"ST2023-022\": \"Connectivity Issue\",\n",
    "    \"ST2023-023\": \"Data Recovery\",\n",
    "    \"ST2023-024\": \"Data Recovery\",\n",
    "    \"ST2023-025\": \"Connectivity Issue\",\n",
    "    \"ST2023-026\": \"Software Issue\"\n",
    "}\n",
    "df['label'] = df['support_tick_id'].map(labels_dict)\n",
    "\n",
    "# Label IDs for fine-tuning\n",
    "label2id = {cat: i for i, cat in enumerate(categories)}\n",
    "df['label_id'] = df['label'].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Accuracy: 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot classification\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def get_top3_zero_shot(text):\n",
    "    result = zero_shot_classifier(text, candidate_labels=categories, multi_label=False)\n",
    "    sorted_labels = [label for _, label in sorted(zip(result['scores'], result['labels']), reverse=True)]\n",
    "    return sorted_labels[:3]\n",
    "\n",
    "# Apply and compute accuracy (top-1)\n",
    "df['zero_shot_top3'] = df['support_ticket_text'].apply(get_top3_zero_shot)\n",
    "df['zero_shot_pred'] = df['zero_shot_top3'].apply(lambda x: x[0])\n",
    "zero_shot_acc = accuracy_score(df['label'], df['zero_shot_pred'])\n",
    "print(f\"Zero-shot Accuracy: {zero_shot_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot Accuracy (FLAN-T5): 0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karat\\AppData\\Local\\Temp\\ipykernel_21004\\54361925.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df['few_shot_top3'] = eval_df['support_ticket_text'].apply(get_top3_few_shot)\n",
      "C:\\Users\\karat\\AppData\\Local\\Temp\\ipykernel_21004\\54361925.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df['few_shot_pred'] = eval_df['few_shot_top3'].apply(lambda x: x[0] if len(x) > 0 else \"\")\n"
     ]
    }
   ],
   "source": [
    "# Few-shot using generative model (FLAN-T5 for local use)\n",
    "# Load FLAN-T5 for text2text-generation\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Few-shot examples (one per category)\n",
    "examples = [\n",
    "    (df[df['support_tick_id'] == 'ST2023-006']['support_ticket_text'].values[0], \"Connectivity Issue\"),\n",
    "    (df[df['support_tick_id'] == 'ST2023-007']['support_ticket_text'].values[0], \"Hardware Malfunction\"),\n",
    "    (df[df['support_tick_id'] == 'ST2023-008']['support_ticket_text'].values[0], \"Data Recovery\"),\n",
    "    (df[df['support_tick_id'] == 'ST2023-010']['support_ticket_text'].values[0], \"Battery Issue\"),\n",
    "    (df[df['support_tick_id'] == 'ST2023-011']['support_ticket_text'].values[0], \"Account Access\"),\n",
    "    (df[df['support_tick_id'] == 'ST2023-012']['support_ticket_text'].values[0], \"Performance Issue\"),\n",
    "    (df[df['support_tick_id'] == 'ST2023-026']['support_ticket_text'].values[0], \"Software Issue\")\n",
    "]\n",
    "\n",
    "# Build few-shot prompt base\n",
    "few_shot_base = \"You are a support ticket classifier. Given a ticket, return the top 3 most probable categories from: \" + \", \".join(categories) + \".\\n\\n\"\n",
    "\n",
    "for text, cat in examples:\n",
    "    few_shot_base += f\"Ticket: {text}\\nTop category: {cat}\\n\\n\"\n",
    "\n",
    "def get_top3_few_shot(text):\n",
    "    prompt = few_shot_base + f\"Ticket: {text}\\nTop 3 categories:\"\n",
    "    output = generator(prompt, max_new_tokens=50)[0]['generated_text']\n",
    "    tags = [t.strip() for t in output.split(\",\")[:3]]\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Apply and compute accuracy (excluding example tickets for fairness)\n",
    "example_ids = [\"ST2023-006\", \"ST2023-007\", \"ST2023-008\", \"ST2023-010\", \"ST2023-011\", \"ST2023-012\", \"ST2023-026\"]\n",
    "eval_df = df[~df['support_tick_id'].isin(example_ids)]\n",
    "eval_df['few_shot_top3'] = eval_df['support_ticket_text'].apply(get_top3_few_shot)\n",
    "eval_df['few_shot_pred'] = eval_df['few_shot_top3'].apply(lambda x: x[0] if len(x) > 0 else \"\")\n",
    "few_shot_acc = accuracy_score(eval_df['label'], eval_df['few_shot_pred'])\n",
    "print(f\"Few-shot Accuracy (FLAN-T5): {few_shot_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16/16 [00:00<00:00, 1574.44 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 903.79 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\karat\\AppData\\Local\\Temp\\ipykernel_21004\\502671086.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\karat\\Downloads\\archive\\tagging-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.780836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.641653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.795600</td>\n",
       "      <td>1.595633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karat\\Downloads\\archive\\tagging-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\karat\\Downloads\\archive\\tagging-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)  # Removed stratify\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"support_ticket_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['support_ticket_text', 'label_id']]).rename_column(\"label_id\", \"labels\")\n",
    "test_ds = Dataset.from_pandas(test_df[['support_ticket_text', 'label_id']]).rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(categories))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Accuracy: 0.4\n",
      "\n",
      "Performance Comparison:\n",
      "Zero-shot Acc: 0.9047619047619048\n",
      "Few-shot Acc: 0.7142857142857143\n",
      "Fine-tuned Acc: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Evaluate fine-tuned model\n",
    "def get_top3_fine_tuned(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "    top3_idx = torch.topk(probs, 3).indices.tolist()\n",
    "    return [categories[i] for i in top3_idx]\n",
    "\n",
    "test_df['fine_tuned_top3'] = test_df['support_ticket_text'].apply(get_top3_fine_tuned)\n",
    "test_df['fine_tuned_pred'] = test_df['fine_tuned_top3'].apply(lambda x: x[0])\n",
    "fine_tuned_acc = accuracy_score(test_df['label'], test_df['fine_tuned_pred'])\n",
    "print(f\"Fine-tuned Accuracy: {fine_tuned_acc}\")\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\nPerformance Comparison:\\nZero-shot Acc: {zero_shot_acc}\\nFew-shot Acc: {few_shot_acc}\\nFine-tuned Acc: {fine_tuned_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tagging-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
